{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:50:37.499662140Z",
     "start_time": "2024-07-29T17:50:37.456099679Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "outputs": [
    {
     "data": {
      "text/plain": "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n\n  LandContour Utilities  ... ScreenPorch PoolArea PoolQC Fence MiscFeature  \\\n0         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n1         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n2         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n3         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n4         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n\n  MiscVal MoSold  YrSold  SaleType  SaleCondition  \n0       0      2    2008        WD         Normal  \n1       0      5    2007        WD         Normal  \n2       0      9    2008        WD         Normal  \n3       0      2    2006        WD        Abnorml  \n4       0     12    2008        WD         Normal  \n\n[5 rows x 80 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>Alley</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>...</th>\n      <th>ScreenPorch</th>\n      <th>PoolArea</th>\n      <th>PoolQC</th>\n      <th>Fence</th>\n      <th>MiscFeature</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 80 columns</p>\n</div>"
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "y = data['SalePrice']\n",
    "X = data.drop('SalePrice', axis=1)\n",
    "X.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:50:37.581192941Z",
     "start_time": "2024-07-29T17:50:37.461012997Z"
    }
   },
   "id": "3f375d6aeed3e16c"
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "outputs": [
    {
     "data": {
      "text/plain": "   Id  MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n0   1          60         65.0     8450            7            5       2003   \n1   2          20         80.0     9600            6            8       1976   \n2   3          60         68.0    11250            7            5       2001   \n3   4          70         60.0     9550            7            5       1915   \n4   5          60         84.0    14260            8            5       2000   \n\n   YearRemodAdd  MasVnrArea  BsmtFinSF1  ...  GarageArea  WoodDeckSF  \\\n0          2003       196.0         706  ...         548           0   \n1          1976         0.0         978  ...         460         298   \n2          2002       162.0         486  ...         608           0   \n3          1970         0.0         216  ...         642           0   \n4          2000       350.0         655  ...         836         192   \n\n   OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  MiscVal  \\\n0           61              0          0            0         0        0   \n1            0              0          0            0         0        0   \n2           42              0          0            0         0        0   \n3           35            272          0            0         0        0   \n4           84              0          0            0         0        0   \n\n   MoSold  YrSold  \n0       2    2008  \n1       5    2007  \n2       9    2008  \n3       2    2006  \n4      12    2008  \n\n[5 rows x 37 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>OverallQual</th>\n      <th>OverallCond</th>\n      <th>YearBuilt</th>\n      <th>YearRemodAdd</th>\n      <th>MasVnrArea</th>\n      <th>BsmtFinSF1</th>\n      <th>...</th>\n      <th>GarageArea</th>\n      <th>WoodDeckSF</th>\n      <th>OpenPorchSF</th>\n      <th>EnclosedPorch</th>\n      <th>3SsnPorch</th>\n      <th>ScreenPorch</th>\n      <th>PoolArea</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>60</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>7</td>\n      <td>5</td>\n      <td>2003</td>\n      <td>2003</td>\n      <td>196.0</td>\n      <td>706</td>\n      <td>...</td>\n      <td>548</td>\n      <td>0</td>\n      <td>61</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2008</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>20</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>6</td>\n      <td>8</td>\n      <td>1976</td>\n      <td>1976</td>\n      <td>0.0</td>\n      <td>978</td>\n      <td>...</td>\n      <td>460</td>\n      <td>298</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>60</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>7</td>\n      <td>5</td>\n      <td>2001</td>\n      <td>2002</td>\n      <td>162.0</td>\n      <td>486</td>\n      <td>...</td>\n      <td>608</td>\n      <td>0</td>\n      <td>42</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2008</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>70</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>7</td>\n      <td>5</td>\n      <td>1915</td>\n      <td>1970</td>\n      <td>0.0</td>\n      <td>216</td>\n      <td>...</td>\n      <td>642</td>\n      <td>0</td>\n      <td>35</td>\n      <td>272</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2006</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>60</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>8</td>\n      <td>5</td>\n      <td>2000</td>\n      <td>2000</td>\n      <td>350.0</td>\n      <td>655</td>\n      <td>...</td>\n      <td>836</td>\n      <td>192</td>\n      <td>84</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2008</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 37 columns</p>\n</div>"
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.drop('Id', axis=1)\n",
    "numeric_cols = list(X.select_dtypes(include=np.number).columns)\n",
    "X_num = X[numeric_cols]\n",
    "X_num.columns = X[numeric_cols].columns\n",
    "X_num.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:50:37.583044854Z",
     "start_time": "2024-07-29T17:50:37.521094039Z"
    }
   },
   "id": "f93d42995bc3b2a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First model will use only numeric values and missing values will be imputed with SimpleImputer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7196d8b03b3bd0e1"
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "my_imputer = SimpleImputer()\n",
    "\n",
    "def get_train_test_data(X, y, test_size):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    \n",
    "    imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "    imputed_X_test = pd.DataFrame(my_imputer.transform(X_test))\n",
    "    \n",
    "    imputed_X_train.columns = X_train.columns\n",
    "    imputed_X_test.columns = X_test.columns\n",
    "    \n",
    "    X_train, X_test = imputed_X_train, imputed_X_test\n",
    "    \n",
    "    X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train.values, dtype=torch.float32).squeeze()\n",
    "    y_test = torch.tensor(y_test.values, dtype=torch.float32).squeeze()\n",
    "    return X_train, X_test, y_train, y_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:50:37.583159771Z",
     "start_time": "2024-07-29T17:50:37.521229998Z"
    }
   },
   "id": "6abb294b85f240c3"
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test_data(X_num, y, test_size=0.2)\n",
    "X_num = pd.DataFrame(my_imputer.transform(X_num))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:50:37.583229373Z",
     "start_time": "2024-07-29T17:50:37.521272963Z"
    }
   },
   "id": "1430845137a86db"
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "outputs": [],
   "source": [
    "class ModelV1(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features=in_features, out_features=1),\n",
    "            # nn.Linear(in_features=10, out_features=10),\n",
    "            # nn.Linear(in_features=10, out_features=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:50:37.583271083Z",
     "start_time": "2024-07-29T17:50:37.521310670Z"
    }
   },
   "id": "f33340deaf7665f7"
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "outputs": [],
   "source": [
    "model1 = ModelV1(in_features=37)\n",
    "\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(params=model1.parameters(), lr=0.0001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:50:37.583311091Z",
     "start_time": "2024-07-29T17:50:37.521400317Z"
    }
   },
   "id": "cfa7ae65ba16ce"
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-268.0148,  -98.6031, -303.8015,  ..., -330.1853, -338.9747,\n        -250.6477], grad_fn=<SqueezeBackward0>)"
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.eval()\n",
    "y_pred = model1(X_train).squeeze()\n",
    "y_pred"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:50:37.583430868Z",
     "start_time": "2024-07-29T17:50:37.521438446Z"
    }
   },
   "id": "1534193b0b21b3c5"
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "outputs": [],
   "source": [
    "def train_test(model:torch.nn.Module, loss_fn:torch.nn.Module, optimizer:torch.optim.Optimizer, epochs:int, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        y_preds = model(X_train).squeeze()\n",
    "        loss = loss_fn(y_preds, y_train)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with (torch.inference_mode()):\n",
    "            test_pred = model(X_test).squeeze()\n",
    "            test_loss = loss_fn(test_pred, y_test)\n",
    "        if epoch % 50 == 0:\n",
    "            print(\"----------------------------\")\n",
    "            print(f\"Epoch {epoch}:\")\n",
    "            print(f\"Train loss: {loss} | Test loss: {test_loss}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:50:37.583713736Z",
     "start_time": "2024-07-29T17:50:37.525907145Z"
    }
   },
   "id": "99076809838fcd50"
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "Epoch 0:\n",
      "Train loss: 180182.03125 | Test loss: 171856.265625\n",
      "----------------------------\n",
      "Epoch 50:\n",
      "Train loss: 58346.25390625 | Test loss: 65808.96875\n",
      "----------------------------\n",
      "Epoch 100:\n",
      "Train loss: 54455.6875 | Test loss: 61339.3359375\n",
      "----------------------------\n",
      "Epoch 150:\n",
      "Train loss: 51197.4375 | Test loss: 57555.8203125\n",
      "----------------------------\n",
      "Epoch 200:\n",
      "Train loss: 48678.0625 | Test loss: 54606.90625\n",
      "----------------------------\n",
      "Epoch 250:\n",
      "Train loss: 46793.92578125 | Test loss: 52240.30859375\n",
      "----------------------------\n",
      "Epoch 300:\n",
      "Train loss: 45321.69921875 | Test loss: 50298.80859375\n",
      "----------------------------\n",
      "Epoch 350:\n",
      "Train loss: 44132.43359375 | Test loss: 48973.6484375\n",
      "----------------------------\n",
      "Epoch 400:\n",
      "Train loss: 43195.69921875 | Test loss: 47893.1171875\n",
      "----------------------------\n",
      "Epoch 450:\n",
      "Train loss: 42339.078125 | Test loss: 46968.4921875\n",
      "----------------------------\n",
      "Epoch 500:\n",
      "Train loss: 41575.4921875 | Test loss: 46161.5703125\n",
      "----------------------------\n",
      "Epoch 550:\n",
      "Train loss: 40860.54296875 | Test loss: 45415.55078125\n",
      "----------------------------\n",
      "Epoch 600:\n",
      "Train loss: 40233.76171875 | Test loss: 44721.73046875\n",
      "----------------------------\n",
      "Epoch 650:\n",
      "Train loss: 39709.8515625 | Test loss: 44096.8671875\n",
      "----------------------------\n",
      "Epoch 700:\n",
      "Train loss: 39217.234375 | Test loss: 43550.07421875\n",
      "----------------------------\n",
      "Epoch 750:\n",
      "Train loss: 38750.94140625 | Test loss: 43027.625\n",
      "----------------------------\n",
      "Epoch 800:\n",
      "Train loss: 38291.36328125 | Test loss: 42517.37109375\n",
      "----------------------------\n",
      "Epoch 850:\n",
      "Train loss: 37845.15234375 | Test loss: 42033.14453125\n",
      "----------------------------\n",
      "Epoch 900:\n",
      "Train loss: 37417.13671875 | Test loss: 41580.91796875\n",
      "----------------------------\n",
      "Epoch 950:\n",
      "Train loss: 37004.7265625 | Test loss: 41153.73828125\n",
      "----------------------------\n",
      "Epoch 1000:\n",
      "Train loss: 36598.91796875 | Test loss: 40730.87109375\n",
      "----------------------------\n",
      "Epoch 1050:\n",
      "Train loss: 36205.65234375 | Test loss: 40300.6484375\n",
      "----------------------------\n",
      "Epoch 1100:\n",
      "Train loss: 35823.27734375 | Test loss: 39892.5703125\n",
      "----------------------------\n",
      "Epoch 1150:\n",
      "Train loss: 35468.31640625 | Test loss: 39522.60546875\n",
      "----------------------------\n",
      "Epoch 1200:\n",
      "Train loss: 35128.41796875 | Test loss: 39164.5546875\n",
      "----------------------------\n",
      "Epoch 1250:\n",
      "Train loss: 34797.5625 | Test loss: 38812.6171875\n",
      "----------------------------\n",
      "Epoch 1300:\n",
      "Train loss: 34476.9375 | Test loss: 38474.95703125\n",
      "----------------------------\n",
      "Epoch 1350:\n",
      "Train loss: 34166.05859375 | Test loss: 38149.15625\n",
      "----------------------------\n",
      "Epoch 1400:\n",
      "Train loss: 33865.58203125 | Test loss: 37847.92578125\n",
      "----------------------------\n",
      "Epoch 1450:\n",
      "Train loss: 33581.20703125 | Test loss: 37564.4921875\n",
      "----------------------------\n",
      "Epoch 1500:\n",
      "Train loss: 33304.765625 | Test loss: 37289.4296875\n",
      "----------------------------\n",
      "Epoch 1550:\n",
      "Train loss: 33039.1640625 | Test loss: 37023.9296875\n",
      "----------------------------\n",
      "Epoch 1600:\n",
      "Train loss: 32790.3203125 | Test loss: 36780.35546875\n",
      "----------------------------\n",
      "Epoch 1650:\n",
      "Train loss: 32554.39453125 | Test loss: 36540.01953125\n",
      "----------------------------\n",
      "Epoch 1700:\n",
      "Train loss: 32329.904296875 | Test loss: 36294.52734375\n",
      "----------------------------\n",
      "Epoch 1750:\n",
      "Train loss: 32113.931640625 | Test loss: 36068.765625\n",
      "----------------------------\n",
      "Epoch 1800:\n",
      "Train loss: 31904.845703125 | Test loss: 35846.6796875\n",
      "----------------------------\n",
      "Epoch 1850:\n",
      "Train loss: 31704.69140625 | Test loss: 35627.234375\n",
      "----------------------------\n",
      "Epoch 1900:\n",
      "Train loss: 31511.171875 | Test loss: 35415.05859375\n",
      "----------------------------\n",
      "Epoch 1950:\n",
      "Train loss: 31323.962890625 | Test loss: 35217.79296875\n"
     ]
    }
   ],
   "source": [
    "train_test(model=model1, loss_fn=loss_fn, optimizer=optimizer, epochs=2000, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:50:38.637483187Z",
     "start_time": "2024-07-29T17:50:37.569167475Z"
    }
   },
   "id": "cf68177afdf2c915"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## This results are not great, let's figure out which 5 data columns have the most corelation with SalePrice and use only them"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86fb12508d61e8e5"
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:50:38.655126364Z",
     "start_time": "2024-07-29T17:50:38.636175833Z"
    }
   },
   "id": "e8e7e783399b8c90"
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "outputs": [
    {
     "data": {
      "text/plain": "4     0.562154\n16    0.483745\n12    0.367210\n27    0.366140\n6     0.365785\n26    0.356768\n13    0.312129\nName: MI Scores, dtype: float64"
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discrete_features = X_num.dtypes == int\n",
    "\n",
    "mi_scores = make_mi_scores(X_num, y, discrete_features=discrete_features)\n",
    "mi_scores[:7]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:50:38.837552162Z",
     "start_time": "2024-07-29T17:50:38.644131574Z"
    }
   },
   "id": "2e7b7d286e2e9220"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's now use only this 7 features in a Model and see if there is any difference in performance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d35d8f0f804fcc5"
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "outputs": [],
   "source": [
    "model2 = ModelV1(in_features=7)\n",
    "lista = list(mi_scores[:7].index)\n",
    "for i in lista:\n",
    "    i -= 1\n",
    "X_num_2 = X_num[lista]\n",
    "X_train, X_test, y_train, y_test = get_train_test_data(X_num_2, y, test_size=0.2)\n",
    "optimizer_2 = torch.optim.SGD(params=model2.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:50:38.837776045Z",
     "start_time": "2024-07-29T17:50:38.827644611Z"
    }
   },
   "id": "669c09d2f00993d0"
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "Epoch 0:\n",
      "Train loss: 181817.484375 | Test loss: 170244.234375\n",
      "----------------------------\n",
      "Epoch 50:\n",
      "Train loss: 38349.5 | Test loss: 38911.47265625\n",
      "----------------------------\n",
      "Epoch 100:\n",
      "Train loss: 36654.87890625 | Test loss: 37045.5234375\n",
      "----------------------------\n",
      "Epoch 150:\n",
      "Train loss: 35221.98828125 | Test loss: 35635.0546875\n",
      "----------------------------\n",
      "Epoch 200:\n",
      "Train loss: 34072.97265625 | Test loss: 34561.73828125\n",
      "----------------------------\n",
      "Epoch 250:\n",
      "Train loss: 33144.19921875 | Test loss: 33714.734375\n",
      "----------------------------\n",
      "Epoch 300:\n",
      "Train loss: 32369.732421875 | Test loss: 33037.2734375\n",
      "----------------------------\n",
      "Epoch 350:\n",
      "Train loss: 31785.99609375 | Test loss: 32536.1171875\n",
      "----------------------------\n",
      "Epoch 400:\n",
      "Train loss: 31370.119140625 | Test loss: 32165.671875\n",
      "----------------------------\n",
      "Epoch 450:\n",
      "Train loss: 31050.96484375 | Test loss: 31867.2734375\n",
      "----------------------------\n",
      "Epoch 500:\n",
      "Train loss: 30797.85546875 | Test loss: 31638.921875\n",
      "----------------------------\n",
      "Epoch 550:\n",
      "Train loss: 30571.19921875 | Test loss: 31440.322265625\n",
      "----------------------------\n",
      "Epoch 600:\n",
      "Train loss: 30387.5078125 | Test loss: 31256.369140625\n",
      "----------------------------\n",
      "Epoch 650:\n",
      "Train loss: 30227.048828125 | Test loss: 31098.36328125\n",
      "----------------------------\n",
      "Epoch 700:\n",
      "Train loss: 30095.6171875 | Test loss: 30990.52734375\n",
      "----------------------------\n",
      "Epoch 750:\n",
      "Train loss: 29984.759765625 | Test loss: 30898.64453125\n",
      "----------------------------\n",
      "Epoch 800:\n",
      "Train loss: 29883.173828125 | Test loss: 30849.458984375\n",
      "----------------------------\n",
      "Epoch 850:\n",
      "Train loss: 29789.6328125 | Test loss: 30774.376953125\n",
      "----------------------------\n",
      "Epoch 900:\n",
      "Train loss: 29701.986328125 | Test loss: 30698.5\n",
      "----------------------------\n",
      "Epoch 950:\n",
      "Train loss: 29624.927734375 | Test loss: 30631.326171875\n",
      "----------------------------\n",
      "Epoch 1000:\n",
      "Train loss: 29554.21875 | Test loss: 30577.44921875\n",
      "----------------------------\n",
      "Epoch 1050:\n",
      "Train loss: 29492.208984375 | Test loss: 30514.544921875\n",
      "----------------------------\n",
      "Epoch 1100:\n",
      "Train loss: 29436.28125 | Test loss: 30449.900390625\n",
      "----------------------------\n",
      "Epoch 1150:\n",
      "Train loss: 29388.4375 | Test loss: 30394.0078125\n",
      "----------------------------\n",
      "Epoch 1200:\n",
      "Train loss: 29345.05078125 | Test loss: 30345.41015625\n",
      "----------------------------\n",
      "Epoch 1250:\n",
      "Train loss: 29303.38671875 | Test loss: 30295.24609375\n",
      "----------------------------\n",
      "Epoch 1300:\n",
      "Train loss: 29262.9921875 | Test loss: 30250.8125\n",
      "----------------------------\n",
      "Epoch 1350:\n",
      "Train loss: 29224.9765625 | Test loss: 30209.7421875\n",
      "----------------------------\n",
      "Epoch 1400:\n",
      "Train loss: 29188.890625 | Test loss: 30172.13671875\n",
      "----------------------------\n",
      "Epoch 1450:\n",
      "Train loss: 29156.4375 | Test loss: 30132.78515625\n",
      "----------------------------\n",
      "Epoch 1500:\n",
      "Train loss: 29128.89453125 | Test loss: 30106.1328125\n",
      "----------------------------\n",
      "Epoch 1550:\n",
      "Train loss: 29103.955078125 | Test loss: 30074.185546875\n",
      "----------------------------\n",
      "Epoch 1600:\n",
      "Train loss: 29079.822265625 | Test loss: 30046.599609375\n",
      "----------------------------\n",
      "Epoch 1650:\n",
      "Train loss: 29055.845703125 | Test loss: 30016.55078125\n",
      "----------------------------\n",
      "Epoch 1700:\n",
      "Train loss: 29033.53515625 | Test loss: 29987.677734375\n",
      "----------------------------\n",
      "Epoch 1750:\n",
      "Train loss: 29012.828125 | Test loss: 29963.3359375\n",
      "----------------------------\n",
      "Epoch 1800:\n",
      "Train loss: 28994.24609375 | Test loss: 29944.0\n",
      "----------------------------\n",
      "Epoch 1850:\n",
      "Train loss: 28976.673828125 | Test loss: 29926.890625\n",
      "----------------------------\n",
      "Epoch 1900:\n",
      "Train loss: 28961.681640625 | Test loss: 29902.52734375\n",
      "----------------------------\n",
      "Epoch 1950:\n",
      "Train loss: 28947.36328125 | Test loss: 29884.11328125\n"
     ]
    }
   ],
   "source": [
    "train_test(model=model2, loss_fn=loss_fn, optimizer=optimizer_2, epochs=2000, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:50:39.245478763Z",
     "start_time": "2024-07-29T17:50:38.836238601Z"
    }
   },
   "id": "84eeee795b86b4c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Now let's try to normalize data and see whether it improves our result or not"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0109b661b6b2c0c"
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "outputs": [],
   "source": [
    "X_num_3 = X_num_2\n",
    "y_normalized = pd.DataFrame(torch.nn.functional.normalize(torch.tensor(y, dtype=torch.float32).unsqueeze(dim=1)))\n",
    "X_train, X_test, y_train, y_test = get_train_test_data(X_num_3, y_normalized, test_size=0.2)\n",
    "X_train, X_test = torch.nn.functional.normalize(X_train), torch.nn.functional.normalize(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:51:01.957584888Z",
     "start_time": "2024-07-29T17:51:01.910067832Z"
    }
   },
   "id": "35286dbc2acaba91"
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "outputs": [],
   "source": [
    "model3 = ModelV1(in_features=7)\n",
    "optimizer_3 = torch.optim.SGD(params=model3.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:51:02.334621015Z",
     "start_time": "2024-07-29T17:51:02.329607111Z"
    }
   },
   "id": "2a561fb63a6745c0"
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "Epoch 0:\n",
      "Train loss: 1.496457815170288 | Test loss: 1.4966332912445068\n",
      "----------------------------\n",
      "Epoch 50:\n",
      "Train loss: 1.3984014987945557 | Test loss: 1.398505687713623\n",
      "----------------------------\n",
      "Epoch 100:\n",
      "Train loss: 1.300345540046692 | Test loss: 1.3003778457641602\n",
      "----------------------------\n",
      "Epoch 150:\n",
      "Train loss: 1.2022892236709595 | Test loss: 1.2022500038146973\n",
      "----------------------------\n",
      "Epoch 200:\n",
      "Train loss: 1.1042330265045166 | Test loss: 1.1041224002838135\n",
      "----------------------------\n",
      "Epoch 250:\n",
      "Train loss: 1.006177306175232 | Test loss: 1.0059951543807983\n",
      "----------------------------\n",
      "Epoch 300:\n",
      "Train loss: 0.9081215858459473 | Test loss: 0.9078678488731384\n",
      "----------------------------\n",
      "Epoch 350:\n",
      "Train loss: 0.8100658059120178 | Test loss: 0.8097404837608337\n",
      "----------------------------\n",
      "Epoch 400:\n",
      "Train loss: 0.7120100259780884 | Test loss: 0.711613118648529\n",
      "----------------------------\n",
      "Epoch 450:\n",
      "Train loss: 0.6139541268348694 | Test loss: 0.6134857535362244\n",
      "----------------------------\n",
      "Epoch 500:\n",
      "Train loss: 0.5158987045288086 | Test loss: 0.5153588652610779\n",
      "----------------------------\n",
      "Epoch 550:\n",
      "Train loss: 0.4178434908390045 | Test loss: 0.41723212599754333\n",
      "----------------------------\n",
      "Epoch 600:\n",
      "Train loss: 0.3197884261608124 | Test loss: 0.3191055357456207\n",
      "----------------------------\n",
      "Epoch 650:\n",
      "Train loss: 0.22173427045345306 | Test loss: 0.22097985446453094\n",
      "----------------------------\n",
      "Epoch 700:\n",
      "Train loss: 0.1239258348941803 | Test loss: 0.12318038940429688\n",
      "----------------------------\n",
      "Epoch 750:\n",
      "Train loss: 0.05534384399652481 | Test loss: 0.0529533214867115\n",
      "----------------------------\n",
      "Epoch 800:\n",
      "Train loss: 0.04621732980012894 | Test loss: 0.043015334755182266\n",
      "----------------------------\n",
      "Epoch 850:\n",
      "Train loss: 0.04517226666212082 | Test loss: 0.04189261794090271\n",
      "----------------------------\n",
      "Epoch 900:\n",
      "Train loss: 0.04454891383647919 | Test loss: 0.041310906410217285\n",
      "----------------------------\n",
      "Epoch 950:\n",
      "Train loss: 0.0439646914601326 | Test loss: 0.04077758267521858\n",
      "----------------------------\n",
      "Epoch 1000:\n",
      "Train loss: 0.043381910771131516 | Test loss: 0.04024782031774521\n",
      "----------------------------\n",
      "Epoch 1050:\n",
      "Train loss: 0.04279917851090431 | Test loss: 0.039718594402074814\n",
      "----------------------------\n",
      "Epoch 1100:\n",
      "Train loss: 0.04221654683351517 | Test loss: 0.03918977826833725\n",
      "----------------------------\n",
      "Epoch 1150:\n",
      "Train loss: 0.0416351780295372 | Test loss: 0.0386604405939579\n",
      "----------------------------\n",
      "Epoch 1200:\n",
      "Train loss: 0.04105418547987938 | Test loss: 0.03813262656331062\n",
      "----------------------------\n",
      "Epoch 1250:\n",
      "Train loss: 0.04047369211912155 | Test loss: 0.03760462626814842\n",
      "----------------------------\n",
      "Epoch 1300:\n",
      "Train loss: 0.03989367559552193 | Test loss: 0.03707721456885338\n",
      "----------------------------\n",
      "Epoch 1350:\n",
      "Train loss: 0.039313651621341705 | Test loss: 0.03654981777071953\n",
      "----------------------------\n",
      "Epoch 1400:\n",
      "Train loss: 0.03873363509774208 | Test loss: 0.03602231293916702\n",
      "----------------------------\n",
      "Epoch 1450:\n",
      "Train loss: 0.03815373405814171 | Test loss: 0.03549309819936752\n",
      "----------------------------\n",
      "Epoch 1500:\n",
      "Train loss: 0.037573833018541336 | Test loss: 0.03496421501040459\n",
      "----------------------------\n",
      "Epoch 1550:\n",
      "Train loss: 0.03699393570423126 | Test loss: 0.03443639725446701\n",
      "----------------------------\n",
      "Epoch 1600:\n",
      "Train loss: 0.03641402721405029 | Test loss: 0.03390880674123764\n",
      "----------------------------\n",
      "Epoch 1650:\n",
      "Train loss: 0.03583412617444992 | Test loss: 0.03338146209716797\n",
      "----------------------------\n",
      "Epoch 1700:\n",
      "Train loss: 0.035254597663879395 | Test loss: 0.03285420686006546\n",
      "----------------------------\n",
      "Epoch 1750:\n",
      "Train loss: 0.034676019102334976 | Test loss: 0.03232757747173309\n",
      "----------------------------\n",
      "Epoch 1800:\n",
      "Train loss: 0.03409785032272339 | Test loss: 0.0318017452955246\n",
      "----------------------------\n",
      "Epoch 1850:\n",
      "Train loss: 0.033521220088005066 | Test loss: 0.03127867728471756\n",
      "----------------------------\n",
      "Epoch 1900:\n",
      "Train loss: 0.03294526785612106 | Test loss: 0.030755499377846718\n",
      "----------------------------\n",
      "Epoch 1950:\n",
      "Train loss: 0.0323696993291378 | Test loss: 0.030231965705752373\n",
      "----------------------------\n",
      "Epoch 2000:\n",
      "Train loss: 0.03179413452744484 | Test loss: 0.029709845781326294\n"
     ]
    }
   ],
   "source": [
    "train_test(model=model3, loss_fn=loss_fn, optimizer=optimizer_3, epochs=2001, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:51:03.149984363Z",
     "start_time": "2024-07-29T17:51:02.737853769Z"
    }
   },
   "id": "4c30bf9e2e26b5e7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's try now with more complicated model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30ebc001f520a21a"
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test_data(X_num, y, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:55:04.410038164Z",
     "start_time": "2024-07-29T17:55:04.399945081Z"
    }
   },
   "id": "bf2afca4710f282b"
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "outputs": [],
   "source": [
    "class ModelV2(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(in_features=in_features, out_features=10),\n",
    "            nn.Linear(in_features=10, out_features=10),\n",
    "            nn.Linear(in_features=10, out_features=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T18:01:21.109171292Z",
     "start_time": "2024-07-29T18:01:21.101955076Z"
    }
   },
   "id": "981238182a99652f"
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "outputs": [],
   "source": [
    "model4 = ModelV2(37)\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer_4 = torch.optim.Adam(model4.parameters(), lr=0.01, weight_decay=1e-5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T18:07:32.997861303Z",
     "start_time": "2024-07-29T18:07:32.989403857Z"
    }
   },
   "id": "7e3837a525559f1d"
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "Epoch 0:\n",
      "Train loss: 181763.859375 | Test loss: 177604.28125\n",
      "----------------------------\n",
      "Epoch 50:\n",
      "Train loss: 49329.9140625 | Test loss: 42873.515625\n",
      "----------------------------\n",
      "Epoch 100:\n",
      "Train loss: 41520.7578125 | Test loss: 34990.2734375\n",
      "----------------------------\n",
      "Epoch 150:\n",
      "Train loss: 33197.30078125 | Test loss: 29182.814453125\n",
      "----------------------------\n",
      "Epoch 200:\n",
      "Train loss: 28842.8125 | Test loss: 25398.689453125\n",
      "----------------------------\n",
      "Epoch 250:\n",
      "Train loss: 28026.83984375 | Test loss: 25108.298828125\n",
      "----------------------------\n",
      "Epoch 300:\n",
      "Train loss: 27781.712890625 | Test loss: 25098.125\n",
      "----------------------------\n",
      "Epoch 350:\n",
      "Train loss: 27619.1171875 | Test loss: 25059.158203125\n",
      "----------------------------\n",
      "Epoch 400:\n",
      "Train loss: 27517.65234375 | Test loss: 24901.06640625\n",
      "----------------------------\n",
      "Epoch 450:\n",
      "Train loss: 27450.36328125 | Test loss: 24839.5\n",
      "----------------------------\n",
      "Epoch 500:\n",
      "Train loss: 27414.349609375 | Test loss: 24752.263671875\n",
      "----------------------------\n",
      "Epoch 550:\n",
      "Train loss: 27388.509765625 | Test loss: 24733.421875\n",
      "----------------------------\n",
      "Epoch 600:\n",
      "Train loss: 27365.88671875 | Test loss: 24695.8359375\n",
      "----------------------------\n",
      "Epoch 650:\n",
      "Train loss: 27345.1328125 | Test loss: 24653.748046875\n",
      "----------------------------\n",
      "Epoch 700:\n",
      "Train loss: 27325.421875 | Test loss: 24611.14453125\n",
      "----------------------------\n",
      "Epoch 750:\n",
      "Train loss: 27306.71484375 | Test loss: 24625.103515625\n",
      "----------------------------\n",
      "Epoch 800:\n",
      "Train loss: 27281.826171875 | Test loss: 24586.197265625\n",
      "----------------------------\n",
      "Epoch 850:\n",
      "Train loss: 27258.046875 | Test loss: 24617.03125\n",
      "----------------------------\n",
      "Epoch 900:\n",
      "Train loss: 27239.2421875 | Test loss: 24553.951171875\n",
      "----------------------------\n",
      "Epoch 950:\n",
      "Train loss: 27206.548828125 | Test loss: 24520.140625\n",
      "----------------------------\n",
      "Epoch 1000:\n",
      "Train loss: 27192.712890625 | Test loss: 24484.013671875\n",
      "----------------------------\n",
      "Epoch 1050:\n",
      "Train loss: 27156.400390625 | Test loss: 24495.712890625\n",
      "----------------------------\n",
      "Epoch 1100:\n",
      "Train loss: 27138.10546875 | Test loss: 24484.146484375\n",
      "----------------------------\n",
      "Epoch 1150:\n",
      "Train loss: 27095.03515625 | Test loss: 24424.41015625\n",
      "----------------------------\n",
      "Epoch 1200:\n",
      "Train loss: 27064.37890625 | Test loss: 24395.595703125\n",
      "----------------------------\n",
      "Epoch 1250:\n",
      "Train loss: 27023.716796875 | Test loss: 24291.912109375\n",
      "----------------------------\n",
      "Epoch 1300:\n",
      "Train loss: 26983.27734375 | Test loss: 24365.71484375\n",
      "----------------------------\n",
      "Epoch 1350:\n",
      "Train loss: 26951.05859375 | Test loss: 24324.396484375\n",
      "----------------------------\n",
      "Epoch 1400:\n",
      "Train loss: 26887.271484375 | Test loss: 24230.404296875\n",
      "----------------------------\n",
      "Epoch 1450:\n",
      "Train loss: 26833.267578125 | Test loss: 24177.982421875\n",
      "----------------------------\n",
      "Epoch 1500:\n",
      "Train loss: 26772.615234375 | Test loss: 24029.30078125\n",
      "----------------------------\n",
      "Epoch 1550:\n",
      "Train loss: 26708.287109375 | Test loss: 24032.994140625\n",
      "----------------------------\n",
      "Epoch 1600:\n",
      "Train loss: 26622.412109375 | Test loss: 23833.322265625\n",
      "----------------------------\n",
      "Epoch 1650:\n",
      "Train loss: 26542.849609375 | Test loss: 23630.33984375\n",
      "----------------------------\n",
      "Epoch 1700:\n",
      "Train loss: 26425.931640625 | Test loss: 23577.771484375\n",
      "----------------------------\n",
      "Epoch 1750:\n",
      "Train loss: 26325.3203125 | Test loss: 23568.392578125\n",
      "----------------------------\n",
      "Epoch 1800:\n",
      "Train loss: 26167.36328125 | Test loss: 23153.578125\n",
      "----------------------------\n",
      "Epoch 1850:\n",
      "Train loss: 25985.546875 | Test loss: 23249.28125\n",
      "----------------------------\n",
      "Epoch 1900:\n",
      "Train loss: 25773.865234375 | Test loss: 22698.435546875\n",
      "----------------------------\n",
      "Epoch 1950:\n",
      "Train loss: 25524.623046875 | Test loss: 22568.78515625\n"
     ]
    }
   ],
   "source": [
    "train_test(model4, loss_fn=loss_fn, optimizer=optimizer_4, epochs=2000, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T18:07:35.951798928Z",
     "start_time": "2024-07-29T18:07:33.696009409Z"
    }
   },
   "id": "ce6c6da6208f2ba9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
