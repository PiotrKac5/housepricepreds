{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:16:35.116032675Z",
     "start_time": "2024-07-29T17:16:35.063874729Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "outputs": [
    {
     "data": {
      "text/plain": "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n\n  LandContour Utilities  ... ScreenPorch PoolArea PoolQC Fence MiscFeature  \\\n0         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n1         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n2         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n3         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n4         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n\n  MiscVal MoSold  YrSold  SaleType  SaleCondition  \n0       0      2    2008        WD         Normal  \n1       0      5    2007        WD         Normal  \n2       0      9    2008        WD         Normal  \n3       0      2    2006        WD        Abnorml  \n4       0     12    2008        WD         Normal  \n\n[5 rows x 80 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>Alley</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>...</th>\n      <th>ScreenPorch</th>\n      <th>PoolArea</th>\n      <th>PoolQC</th>\n      <th>Fence</th>\n      <th>MiscFeature</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 80 columns</p>\n</div>"
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "y = data['SalePrice']\n",
    "X = data.drop('SalePrice', axis=1)\n",
    "X.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:16:35.196629525Z",
     "start_time": "2024-07-29T17:16:35.069759390Z"
    }
   },
   "id": "3f375d6aeed3e16c"
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "outputs": [
    {
     "data": {
      "text/plain": "   Id  MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n0   1          60         65.0     8450            7            5       2003   \n1   2          20         80.0     9600            6            8       1976   \n2   3          60         68.0    11250            7            5       2001   \n3   4          70         60.0     9550            7            5       1915   \n4   5          60         84.0    14260            8            5       2000   \n\n   YearRemodAdd  MasVnrArea  BsmtFinSF1  ...  GarageArea  WoodDeckSF  \\\n0          2003       196.0         706  ...         548           0   \n1          1976         0.0         978  ...         460         298   \n2          2002       162.0         486  ...         608           0   \n3          1970         0.0         216  ...         642           0   \n4          2000       350.0         655  ...         836         192   \n\n   OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  MiscVal  \\\n0           61              0          0            0         0        0   \n1            0              0          0            0         0        0   \n2           42              0          0            0         0        0   \n3           35            272          0            0         0        0   \n4           84              0          0            0         0        0   \n\n   MoSold  YrSold  \n0       2    2008  \n1       5    2007  \n2       9    2008  \n3       2    2006  \n4      12    2008  \n\n[5 rows x 37 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>OverallQual</th>\n      <th>OverallCond</th>\n      <th>YearBuilt</th>\n      <th>YearRemodAdd</th>\n      <th>MasVnrArea</th>\n      <th>BsmtFinSF1</th>\n      <th>...</th>\n      <th>GarageArea</th>\n      <th>WoodDeckSF</th>\n      <th>OpenPorchSF</th>\n      <th>EnclosedPorch</th>\n      <th>3SsnPorch</th>\n      <th>ScreenPorch</th>\n      <th>PoolArea</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>60</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>7</td>\n      <td>5</td>\n      <td>2003</td>\n      <td>2003</td>\n      <td>196.0</td>\n      <td>706</td>\n      <td>...</td>\n      <td>548</td>\n      <td>0</td>\n      <td>61</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2008</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>20</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>6</td>\n      <td>8</td>\n      <td>1976</td>\n      <td>1976</td>\n      <td>0.0</td>\n      <td>978</td>\n      <td>...</td>\n      <td>460</td>\n      <td>298</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>60</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>7</td>\n      <td>5</td>\n      <td>2001</td>\n      <td>2002</td>\n      <td>162.0</td>\n      <td>486</td>\n      <td>...</td>\n      <td>608</td>\n      <td>0</td>\n      <td>42</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2008</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>70</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>7</td>\n      <td>5</td>\n      <td>1915</td>\n      <td>1970</td>\n      <td>0.0</td>\n      <td>216</td>\n      <td>...</td>\n      <td>642</td>\n      <td>0</td>\n      <td>35</td>\n      <td>272</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2006</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>60</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>8</td>\n      <td>5</td>\n      <td>2000</td>\n      <td>2000</td>\n      <td>350.0</td>\n      <td>655</td>\n      <td>...</td>\n      <td>836</td>\n      <td>192</td>\n      <td>84</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2008</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 37 columns</p>\n</div>"
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.drop('Id', axis=1)\n",
    "numeric_cols = list(X.select_dtypes(include=np.number).columns)\n",
    "X_num = X[numeric_cols]\n",
    "X_num.columns = X[numeric_cols].columns\n",
    "X_num.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:16:35.199330761Z",
     "start_time": "2024-07-29T17:16:35.133136245Z"
    }
   },
   "id": "f93d42995bc3b2a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First model will use only numeric values and missing values will be imputed with SimpleImputer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7196d8b03b3bd0e1"
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "my_imputer = SimpleImputer()\n",
    "\n",
    "def get_train_test_data(X, y, test_size):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    \n",
    "    imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "    imputed_X_test = pd.DataFrame(my_imputer.transform(X_test))\n",
    "    \n",
    "    imputed_X_train.columns = X_train.columns\n",
    "    imputed_X_test.columns = X_test.columns\n",
    "    \n",
    "    X_train, X_test = imputed_X_train, imputed_X_test\n",
    "    \n",
    "    X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train.values, dtype=torch.float32).squeeze()\n",
    "    y_test = torch.tensor(y_test.values, dtype=torch.float32).squeeze()\n",
    "    return X_train, X_test, y_train, y_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:16:35.199540411Z",
     "start_time": "2024-07-29T17:16:35.133299318Z"
    }
   },
   "id": "6abb294b85f240c3"
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test_data(X_num, y, test_size=0.2)\n",
    "X_num = pd.DataFrame(my_imputer.transform(X_num))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:16:35.199645028Z",
     "start_time": "2024-07-29T17:16:35.133355240Z"
    }
   },
   "id": "1430845137a86db"
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "outputs": [],
   "source": [
    "class ModelV1(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features=in_features, out_features=1),\n",
    "            # nn.Linear(in_features=10, out_features=10),\n",
    "            # nn.Linear(in_features=10, out_features=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:16:35.199704217Z",
     "start_time": "2024-07-29T17:16:35.133403800Z"
    }
   },
   "id": "f33340deaf7665f7"
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "outputs": [],
   "source": [
    "model1 = ModelV1(in_features=37)\n",
    "\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(params=model1.parameters(), lr=0.0001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:16:35.199756245Z",
     "start_time": "2024-07-29T17:16:35.133508403Z"
    }
   },
   "id": "cfa7ae65ba16ce"
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([668.1719, 341.4703, 610.5351,  ..., 294.5693, 132.4700, 317.1259],\n       grad_fn=<SqueezeBackward0>)"
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.eval()\n",
    "y_pred = model1(X_train).squeeze()\n",
    "y_pred"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:16:35.199985172Z",
     "start_time": "2024-07-29T17:16:35.135926637Z"
    }
   },
   "id": "1534193b0b21b3c5"
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "outputs": [],
   "source": [
    "def train_test(model:torch.nn.Module, loss_fn:torch.nn.Module, optimizer:torch.optim.Optimizer, epochs:int, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        y_preds = model(X_train).squeeze()\n",
    "        loss = loss_fn(y_preds, y_train)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with (torch.inference_mode()):\n",
    "            test_pred = model(X_test).squeeze()\n",
    "            test_loss = loss_fn(test_pred, y_test)\n",
    "        if epoch % 50 == 0:\n",
    "            print(\"----------------------------\")\n",
    "            print(f\"Epoch {epoch}:\")\n",
    "            print(f\"Train loss: {loss} | Test loss: {test_loss}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:16:35.201000252Z",
     "start_time": "2024-07-29T17:16:35.144031281Z"
    }
   },
   "id": "99076809838fcd50"
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "Epoch 0:\n",
      "Train loss: 181328.359375 | Test loss: 163381.640625\n",
      "----------------------------\n",
      "Epoch 50:\n",
      "Train loss: 60191.76171875 | Test loss: 57289.18359375\n",
      "----------------------------\n",
      "Epoch 100:\n",
      "Train loss: 55971.21875 | Test loss: 53504.421875\n",
      "----------------------------\n",
      "Epoch 150:\n",
      "Train loss: 52445.51171875 | Test loss: 50371.92578125\n",
      "----------------------------\n",
      "Epoch 200:\n",
      "Train loss: 49821.9921875 | Test loss: 48042.23828125\n",
      "----------------------------\n",
      "Epoch 250:\n",
      "Train loss: 47865.40625 | Test loss: 46192.59765625\n",
      "----------------------------\n",
      "Epoch 300:\n",
      "Train loss: 46356.34765625 | Test loss: 44731.65234375\n",
      "----------------------------\n",
      "Epoch 350:\n",
      "Train loss: 45232.9375 | Test loss: 43667.5703125\n",
      "----------------------------\n",
      "Epoch 400:\n",
      "Train loss: 44301.7421875 | Test loss: 42774.76171875\n",
      "----------------------------\n",
      "Epoch 450:\n",
      "Train loss: 43458.4453125 | Test loss: 41960.04296875\n",
      "----------------------------\n",
      "Epoch 500:\n",
      "Train loss: 42724.9375 | Test loss: 41227.234375\n",
      "----------------------------\n",
      "Epoch 550:\n",
      "Train loss: 42052.65625 | Test loss: 40529.7265625\n",
      "----------------------------\n",
      "Epoch 600:\n",
      "Train loss: 41485.640625 | Test loss: 39949.58203125\n",
      "----------------------------\n",
      "Epoch 650:\n",
      "Train loss: 40962.8359375 | Test loss: 39452.41796875\n",
      "----------------------------\n",
      "Epoch 700:\n",
      "Train loss: 40479.38671875 | Test loss: 38992.3203125\n",
      "----------------------------\n",
      "Epoch 750:\n",
      "Train loss: 40011.375 | Test loss: 38539.7890625\n",
      "----------------------------\n",
      "Epoch 800:\n",
      "Train loss: 39556.9921875 | Test loss: 38085.0546875\n",
      "----------------------------\n",
      "Epoch 850:\n",
      "Train loss: 39118.8203125 | Test loss: 37648.234375\n",
      "----------------------------\n",
      "Epoch 900:\n",
      "Train loss: 38694.34765625 | Test loss: 37236.6796875\n",
      "----------------------------\n",
      "Epoch 950:\n",
      "Train loss: 38279.63671875 | Test loss: 36816.7421875\n",
      "----------------------------\n",
      "Epoch 1000:\n",
      "Train loss: 37874.8046875 | Test loss: 36392.3359375\n",
      "----------------------------\n",
      "Epoch 1050:\n",
      "Train loss: 37479.37890625 | Test loss: 35950.89453125\n",
      "----------------------------\n",
      "Epoch 1100:\n",
      "Train loss: 37086.31640625 | Test loss: 35570.890625\n",
      "----------------------------\n",
      "Epoch 1150:\n",
      "Train loss: 36722.23828125 | Test loss: 35199.12109375\n",
      "----------------------------\n",
      "Epoch 1200:\n",
      "Train loss: 36380.78125 | Test loss: 34821.06640625\n",
      "----------------------------\n",
      "Epoch 1250:\n",
      "Train loss: 36048.875 | Test loss: 34478.09765625\n",
      "----------------------------\n",
      "Epoch 1300:\n",
      "Train loss: 35723.88671875 | Test loss: 34156.203125\n",
      "----------------------------\n",
      "Epoch 1350:\n",
      "Train loss: 35413.53125 | Test loss: 33804.16015625\n",
      "----------------------------\n",
      "Epoch 1400:\n",
      "Train loss: 35104.68359375 | Test loss: 33488.2890625\n",
      "----------------------------\n",
      "Epoch 1450:\n",
      "Train loss: 34804.265625 | Test loss: 33182.1875\n",
      "----------------------------\n",
      "Epoch 1500:\n",
      "Train loss: 34522.3046875 | Test loss: 32913.125\n",
      "----------------------------\n",
      "Epoch 1550:\n",
      "Train loss: 34248.7578125 | Test loss: 32652.212890625\n",
      "----------------------------\n",
      "Epoch 1600:\n",
      "Train loss: 33989.8671875 | Test loss: 32376.71875\n",
      "----------------------------\n",
      "Epoch 1650:\n",
      "Train loss: 33742.15625 | Test loss: 32131.171875\n",
      "----------------------------\n",
      "Epoch 1700:\n",
      "Train loss: 33506.86328125 | Test loss: 31872.24609375\n",
      "----------------------------\n",
      "Epoch 1750:\n",
      "Train loss: 33285.984375 | Test loss: 31642.83984375\n",
      "----------------------------\n",
      "Epoch 1800:\n",
      "Train loss: 33071.8984375 | Test loss: 31458.376953125\n",
      "----------------------------\n",
      "Epoch 1850:\n",
      "Train loss: 32869.74609375 | Test loss: 31232.037109375\n",
      "----------------------------\n",
      "Epoch 1900:\n",
      "Train loss: 32672.486328125 | Test loss: 31008.72265625\n",
      "----------------------------\n",
      "Epoch 1950:\n",
      "Train loss: 32478.130859375 | Test loss: 30796.1171875\n"
     ]
    }
   ],
   "source": [
    "train_test(model=model1, loss_fn=loss_fn, optimizer=optimizer, epochs=2000, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:16:35.982765583Z",
     "start_time": "2024-07-29T17:16:35.189153503Z"
    }
   },
   "id": "cf68177afdf2c915"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## This results are not great, let's figure out which 5 data columns have the most corelation with SalePrice and use only them"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86fb12508d61e8e5"
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:16:35.983089547Z",
     "start_time": "2024-07-29T17:16:35.970546339Z"
    }
   },
   "id": "e8e7e783399b8c90"
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "outputs": [
    {
     "data": {
      "text/plain": "4     0.559256\n16    0.480458\n6     0.370888\n12    0.365883\n27    0.363714\n26    0.359286\n13    0.309015\nName: MI Scores, dtype: float64"
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discrete_features = X_num.dtypes == int\n",
    "\n",
    "mi_scores = make_mi_scores(X_num, y, discrete_features=discrete_features)\n",
    "mi_scores[:7]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:16:36.168911219Z",
     "start_time": "2024-07-29T17:16:35.973716509Z"
    }
   },
   "id": "2e7b7d286e2e9220"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's now use only this 7 features in a Model and see if there is any difference in performance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d35d8f0f804fcc5"
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "outputs": [],
   "source": [
    "model2 = ModelV1(in_features=7)\n",
    "lista = list(mi_scores[:7].index)\n",
    "for i in lista:\n",
    "    i -= 1\n",
    "X_num_2 = X_num[lista]\n",
    "X_train, X_test, y_train, y_test = get_train_test_data(X_num_2, y, test_size=0.2)\n",
    "optimizer_2 = torch.optim.SGD(params=model2.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:16:36.169188977Z",
     "start_time": "2024-07-29T17:16:36.160397005Z"
    }
   },
   "id": "669c09d2f00993d0"
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "Epoch 0:\n",
      "Train loss: 182127.25 | Test loss: 173775.984375\n",
      "----------------------------\n",
      "Epoch 50:\n",
      "Train loss: 37753.9921875 | Test loss: 41764.93359375\n",
      "----------------------------\n",
      "Epoch 100:\n",
      "Train loss: 36077.8359375 | Test loss: 39832.43359375\n",
      "----------------------------\n",
      "Epoch 150:\n",
      "Train loss: 34681.5859375 | Test loss: 38329.70703125\n",
      "----------------------------\n",
      "Epoch 200:\n",
      "Train loss: 33595.33984375 | Test loss: 37123.32421875\n",
      "----------------------------\n",
      "Epoch 250:\n",
      "Train loss: 32751.048828125 | Test loss: 36143.328125\n",
      "----------------------------\n",
      "Epoch 300:\n",
      "Train loss: 32031.763671875 | Test loss: 35308.5546875\n",
      "----------------------------\n",
      "Epoch 350:\n",
      "Train loss: 31488.30078125 | Test loss: 34656.73828125\n",
      "----------------------------\n",
      "Epoch 400:\n",
      "Train loss: 31080.380859375 | Test loss: 34142.70703125\n",
      "----------------------------\n",
      "Epoch 450:\n",
      "Train loss: 30755.951171875 | Test loss: 33785.98046875\n",
      "----------------------------\n",
      "Epoch 500:\n",
      "Train loss: 30483.623046875 | Test loss: 33523.1796875\n",
      "----------------------------\n",
      "Epoch 550:\n",
      "Train loss: 30249.767578125 | Test loss: 33304.5234375\n",
      "----------------------------\n",
      "Epoch 600:\n",
      "Train loss: 30046.1640625 | Test loss: 33104.90625\n",
      "----------------------------\n",
      "Epoch 650:\n",
      "Train loss: 29883.490234375 | Test loss: 32929.75390625\n",
      "----------------------------\n",
      "Epoch 700:\n",
      "Train loss: 29761.041015625 | Test loss: 32782.3359375\n",
      "----------------------------\n",
      "Epoch 750:\n",
      "Train loss: 29660.513671875 | Test loss: 32650.58984375\n",
      "----------------------------\n",
      "Epoch 800:\n",
      "Train loss: 29575.03515625 | Test loss: 32529.931640625\n",
      "----------------------------\n",
      "Epoch 850:\n",
      "Train loss: 29497.9453125 | Test loss: 32414.931640625\n",
      "----------------------------\n",
      "Epoch 900:\n",
      "Train loss: 29427.736328125 | Test loss: 32312.509765625\n",
      "----------------------------\n",
      "Epoch 950:\n",
      "Train loss: 29363.259765625 | Test loss: 32214.03515625\n",
      "----------------------------\n",
      "Epoch 1000:\n",
      "Train loss: 29303.048828125 | Test loss: 32122.2734375\n",
      "----------------------------\n",
      "Epoch 1050:\n",
      "Train loss: 29248.7421875 | Test loss: 32039.423828125\n",
      "----------------------------\n",
      "Epoch 1100:\n",
      "Train loss: 29200.103515625 | Test loss: 31966.259765625\n",
      "----------------------------\n",
      "Epoch 1150:\n",
      "Train loss: 29152.763671875 | Test loss: 31894.908203125\n",
      "----------------------------\n",
      "Epoch 1200:\n",
      "Train loss: 29106.345703125 | Test loss: 31827.3046875\n",
      "----------------------------\n",
      "Epoch 1250:\n",
      "Train loss: 29060.759765625 | Test loss: 31766.0625\n",
      "----------------------------\n",
      "Epoch 1300:\n",
      "Train loss: 29015.455078125 | Test loss: 31707.521484375\n",
      "----------------------------\n",
      "Epoch 1350:\n",
      "Train loss: 28970.986328125 | Test loss: 31648.83203125\n",
      "----------------------------\n",
      "Epoch 1400:\n",
      "Train loss: 28929.908203125 | Test loss: 31598.130859375\n",
      "----------------------------\n",
      "Epoch 1450:\n",
      "Train loss: 28889.55078125 | Test loss: 31554.123046875\n",
      "----------------------------\n",
      "Epoch 1500:\n",
      "Train loss: 28850.478515625 | Test loss: 31510.21484375\n",
      "----------------------------\n",
      "Epoch 1550:\n",
      "Train loss: 28812.60546875 | Test loss: 31471.71875\n",
      "----------------------------\n",
      "Epoch 1600:\n",
      "Train loss: 28779.328125 | Test loss: 31433.478515625\n",
      "----------------------------\n",
      "Epoch 1650:\n",
      "Train loss: 28751.400390625 | Test loss: 31398.091796875\n",
      "----------------------------\n",
      "Epoch 1700:\n",
      "Train loss: 28725.265625 | Test loss: 31362.50390625\n",
      "----------------------------\n",
      "Epoch 1750:\n",
      "Train loss: 28701.890625 | Test loss: 31332.24609375\n",
      "----------------------------\n",
      "Epoch 1800:\n",
      "Train loss: 28682.72265625 | Test loss: 31307.369140625\n",
      "----------------------------\n",
      "Epoch 1850:\n",
      "Train loss: 28666.240234375 | Test loss: 31284.951171875\n",
      "----------------------------\n",
      "Epoch 1900:\n",
      "Train loss: 28650.326171875 | Test loss: 31263.2421875\n",
      "----------------------------\n",
      "Epoch 1950:\n",
      "Train loss: 28635.16015625 | Test loss: 31243.380859375\n"
     ]
    }
   ],
   "source": [
    "train_test(model=model2, loss_fn=loss_fn, optimizer=optimizer_2, epochs=2000, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:16:36.577932614Z",
     "start_time": "2024-07-29T17:16:36.167500871Z"
    }
   },
   "id": "84eeee795b86b4c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Now let's try to normalize data and see whether it improves our result or not"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0109b661b6b2c0c"
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "outputs": [],
   "source": [
    "X_num_3 = X_num_2\n",
    "y_normalized = pd.DataFrame(torch.nn.functional.normalize(torch.tensor(y, dtype=torch.float32).unsqueeze(dim=1)))\n",
    "X_train, X_test, y_train, y_test = get_train_test_data(X_num_3, y_normalized, test_size=0.2)\n",
    "X_train, X_test = torch.nn.functional.normalize(X_train), torch.nn.functional.normalize(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:16:36.578512903Z",
     "start_time": "2024-07-29T17:16:36.573583273Z"
    }
   },
   "id": "35286dbc2acaba91"
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "outputs": [],
   "source": [
    "model3 = ModelV1(in_features=7)\n",
    "optimizer_3 = torch.optim.SGD(params=model3.parameters(), lr=0.0001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:18:40.917846130Z",
     "start_time": "2024-07-29T17:18:40.871085274Z"
    }
   },
   "id": "2a561fb63a6745c0"
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "Epoch 0:\n",
      "Train loss: 0.4545436501502991 | Test loss: 0.45776137709617615\n",
      "----------------------------\n",
      "Epoch 50:\n",
      "Train loss: 0.444738507270813 | Test loss: 0.44794961810112\n",
      "----------------------------\n",
      "Epoch 100:\n",
      "Train loss: 0.43493330478668213 | Test loss: 0.43813785910606384\n",
      "----------------------------\n",
      "Epoch 150:\n",
      "Train loss: 0.42512819170951843 | Test loss: 0.4283260703086853\n",
      "----------------------------\n",
      "Epoch 200:\n",
      "Train loss: 0.41532304883003235 | Test loss: 0.41851434111595154\n",
      "----------------------------\n",
      "Epoch 250:\n",
      "Train loss: 0.40551790595054626 | Test loss: 0.4087026119232178\n",
      "----------------------------\n",
      "Epoch 300:\n",
      "Train loss: 0.39571279287338257 | Test loss: 0.398890882730484\n",
      "----------------------------\n",
      "Epoch 350:\n",
      "Train loss: 0.38590767979621887 | Test loss: 0.38907915353775024\n",
      "----------------------------\n",
      "Epoch 400:\n",
      "Train loss: 0.3761025667190552 | Test loss: 0.37926745414733887\n",
      "----------------------------\n",
      "Epoch 450:\n",
      "Train loss: 0.36629748344421387 | Test loss: 0.3694557249546051\n",
      "----------------------------\n",
      "Epoch 500:\n",
      "Train loss: 0.3564923405647278 | Test loss: 0.35964396595954895\n",
      "----------------------------\n",
      "Epoch 550:\n",
      "Train loss: 0.3466872274875641 | Test loss: 0.3498322069644928\n",
      "----------------------------\n",
      "Epoch 600:\n",
      "Train loss: 0.336882084608078 | Test loss: 0.3400205075740814\n",
      "----------------------------\n",
      "Epoch 650:\n",
      "Train loss: 0.3270769417285919 | Test loss: 0.33020877838134766\n",
      "----------------------------\n",
      "Epoch 700:\n",
      "Train loss: 0.3172718584537506 | Test loss: 0.3203970789909363\n",
      "----------------------------\n",
      "Epoch 750:\n",
      "Train loss: 0.3074667751789093 | Test loss: 0.3105853199958801\n",
      "----------------------------\n",
      "Epoch 800:\n",
      "Train loss: 0.2976616621017456 | Test loss: 0.30077359080314636\n",
      "----------------------------\n",
      "Epoch 850:\n",
      "Train loss: 0.2878565192222595 | Test loss: 0.290961891412735\n",
      "----------------------------\n",
      "Epoch 900:\n",
      "Train loss: 0.2780514061450958 | Test loss: 0.28115013241767883\n",
      "----------------------------\n",
      "Epoch 950:\n",
      "Train loss: 0.2682463228702545 | Test loss: 0.27133840322494507\n",
      "----------------------------\n",
      "Epoch 1000:\n",
      "Train loss: 0.25844117999076843 | Test loss: 0.2615266442298889\n",
      "----------------------------\n",
      "Epoch 1050:\n",
      "Train loss: 0.24863606691360474 | Test loss: 0.2517149746417999\n",
      "----------------------------\n",
      "Epoch 1100:\n",
      "Train loss: 0.23883093893527985 | Test loss: 0.24190323054790497\n",
      "----------------------------\n",
      "Epoch 1150:\n",
      "Train loss: 0.22902582585811615 | Test loss: 0.2320915162563324\n",
      "----------------------------\n",
      "Epoch 1200:\n",
      "Train loss: 0.21922069787979126 | Test loss: 0.22227974236011505\n",
      "----------------------------\n",
      "Epoch 1250:\n",
      "Train loss: 0.20941559970378876 | Test loss: 0.21246802806854248\n",
      "----------------------------\n",
      "Epoch 1300:\n",
      "Train loss: 0.19961047172546387 | Test loss: 0.20265629887580872\n",
      "----------------------------\n",
      "Epoch 1350:\n",
      "Train loss: 0.18980535864830017 | Test loss: 0.19284455478191376\n",
      "----------------------------\n",
      "Epoch 1400:\n",
      "Train loss: 0.18000023066997528 | Test loss: 0.18303285539150238\n",
      "----------------------------\n",
      "Epoch 1450:\n",
      "Train loss: 0.17019511759281158 | Test loss: 0.17322111129760742\n",
      "----------------------------\n",
      "Epoch 1500:\n",
      "Train loss: 0.1603899896144867 | Test loss: 0.16340938210487366\n",
      "----------------------------\n",
      "Epoch 1550:\n",
      "Train loss: 0.1505902111530304 | Test loss: 0.15360040962696075\n",
      "----------------------------\n",
      "Epoch 1600:\n",
      "Train loss: 0.14081653952598572 | Test loss: 0.14380404353141785\n",
      "----------------------------\n",
      "Epoch 1650:\n",
      "Train loss: 0.13104288280010223 | Test loss: 0.13400763273239136\n",
      "----------------------------\n",
      "Epoch 1700:\n",
      "Train loss: 0.12126925587654114 | Test loss: 0.12421157211065292\n",
      "----------------------------\n",
      "Epoch 1750:\n",
      "Train loss: 0.11152848601341248 | Test loss: 0.11443176120519638\n",
      "----------------------------\n",
      "Epoch 1800:\n",
      "Train loss: 0.10182856023311615 | Test loss: 0.1046733558177948\n",
      "----------------------------\n",
      "Epoch 1850:\n",
      "Train loss: 0.09224022924900055 | Test loss: 0.09506335109472275\n",
      "----------------------------\n",
      "Epoch 1900:\n",
      "Train loss: 0.08282921463251114 | Test loss: 0.08565066009759903\n",
      "----------------------------\n",
      "Epoch 1950:\n",
      "Train loss: 0.07402170449495316 | Test loss: 0.0767132043838501\n",
      "----------------------------\n",
      "Epoch 2000:\n",
      "Train loss: 0.06625062972307205 | Test loss: 0.06864584237337112\n"
     ]
    }
   ],
   "source": [
    "train_test(model=model3, loss_fn=loss_fn, optimizer=optimizer_3, epochs=2001, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T17:18:42.218645440Z",
     "start_time": "2024-07-29T17:18:41.793077530Z"
    }
   },
   "id": "4c30bf9e2e26b5e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4e418d1b18820502"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "135d4323e7e3247c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
