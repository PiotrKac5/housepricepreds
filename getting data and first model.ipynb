{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-29T16:10:17.723296937Z",
     "start_time": "2024-07-29T16:10:17.692389665Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n\n  LandContour Utilities  ... ScreenPorch PoolArea PoolQC Fence MiscFeature  \\\n0         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n1         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n2         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n3         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n4         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n\n  MiscVal MoSold  YrSold  SaleType  SaleCondition  \n0       0      2    2008        WD         Normal  \n1       0      5    2007        WD         Normal  \n2       0      9    2008        WD         Normal  \n3       0      2    2006        WD        Abnorml  \n4       0     12    2008        WD         Normal  \n\n[5 rows x 80 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>Alley</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>...</th>\n      <th>ScreenPorch</th>\n      <th>PoolArea</th>\n      <th>PoolQC</th>\n      <th>Fence</th>\n      <th>MiscFeature</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 80 columns</p>\n</div>"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "y = data['SalePrice']\n",
    "X = data.drop('SalePrice', axis=1)\n",
    "X.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T16:10:17.802950774Z",
     "start_time": "2024-07-29T16:10:17.696764867Z"
    }
   },
   "id": "3f375d6aeed3e16c"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "   Id  MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n0   1          60         65.0     8450            7            5       2003   \n1   2          20         80.0     9600            6            8       1976   \n2   3          60         68.0    11250            7            5       2001   \n3   4          70         60.0     9550            7            5       1915   \n4   5          60         84.0    14260            8            5       2000   \n\n   YearRemodAdd  MasVnrArea  BsmtFinSF1  ...  GarageArea  WoodDeckSF  \\\n0          2003       196.0         706  ...         548           0   \n1          1976         0.0         978  ...         460         298   \n2          2002       162.0         486  ...         608           0   \n3          1970         0.0         216  ...         642           0   \n4          2000       350.0         655  ...         836         192   \n\n   OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  MiscVal  \\\n0           61              0          0            0         0        0   \n1            0              0          0            0         0        0   \n2           42              0          0            0         0        0   \n3           35            272          0            0         0        0   \n4           84              0          0            0         0        0   \n\n   MoSold  YrSold  \n0       2    2008  \n1       5    2007  \n2       9    2008  \n3       2    2006  \n4      12    2008  \n\n[5 rows x 37 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>OverallQual</th>\n      <th>OverallCond</th>\n      <th>YearBuilt</th>\n      <th>YearRemodAdd</th>\n      <th>MasVnrArea</th>\n      <th>BsmtFinSF1</th>\n      <th>...</th>\n      <th>GarageArea</th>\n      <th>WoodDeckSF</th>\n      <th>OpenPorchSF</th>\n      <th>EnclosedPorch</th>\n      <th>3SsnPorch</th>\n      <th>ScreenPorch</th>\n      <th>PoolArea</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>60</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>7</td>\n      <td>5</td>\n      <td>2003</td>\n      <td>2003</td>\n      <td>196.0</td>\n      <td>706</td>\n      <td>...</td>\n      <td>548</td>\n      <td>0</td>\n      <td>61</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2008</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>20</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>6</td>\n      <td>8</td>\n      <td>1976</td>\n      <td>1976</td>\n      <td>0.0</td>\n      <td>978</td>\n      <td>...</td>\n      <td>460</td>\n      <td>298</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>60</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>7</td>\n      <td>5</td>\n      <td>2001</td>\n      <td>2002</td>\n      <td>162.0</td>\n      <td>486</td>\n      <td>...</td>\n      <td>608</td>\n      <td>0</td>\n      <td>42</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2008</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>70</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>7</td>\n      <td>5</td>\n      <td>1915</td>\n      <td>1970</td>\n      <td>0.0</td>\n      <td>216</td>\n      <td>...</td>\n      <td>642</td>\n      <td>0</td>\n      <td>35</td>\n      <td>272</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2006</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>60</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>8</td>\n      <td>5</td>\n      <td>2000</td>\n      <td>2000</td>\n      <td>350.0</td>\n      <td>655</td>\n      <td>...</td>\n      <td>836</td>\n      <td>192</td>\n      <td>84</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2008</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 37 columns</p>\n</div>"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.drop('Id', axis=1)\n",
    "numeric_cols = list(X.select_dtypes(include=np.number).columns)\n",
    "X_num = X[numeric_cols]\n",
    "X_num.columns = X[numeric_cols].columns\n",
    "X_num.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T16:10:17.804391882Z",
     "start_time": "2024-07-29T16:10:17.765246996Z"
    }
   },
   "id": "f93d42995bc3b2a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First model will use only numeric values and missing values will be imputed with SimpleImputer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7196d8b03b3bd0e1"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "my_imputer = SimpleImputer()\n",
    "\n",
    "def get_train_test_data(X, y, test_size):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    \n",
    "    imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "    imputed_X_test = pd.DataFrame(my_imputer.transform(X_test))\n",
    "    \n",
    "    imputed_X_train.columns = X_train.columns\n",
    "    imputed_X_test.columns = X_test.columns\n",
    "    \n",
    "    X_train, X_test = imputed_X_train, imputed_X_test\n",
    "    \n",
    "    X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "    return X_train, X_test, y_train, y_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T16:10:17.804558068Z",
     "start_time": "2024-07-29T16:10:17.765478267Z"
    }
   },
   "id": "6abb294b85f240c3"
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test_data(X_num, y, test_size=0.2)\n",
    "X_num = pd.DataFrame(my_imputer.transform(X_num))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T16:22:23.981944669Z",
     "start_time": "2024-07-29T16:22:23.937504612Z"
    }
   },
   "id": "1430845137a86db"
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "class ModelV1(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features=in_features, out_features=1),\n",
    "            # nn.Linear(in_features=10, out_features=10),\n",
    "            # nn.Linear(in_features=10, out_features=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T16:22:59.021568926Z",
     "start_time": "2024-07-29T16:22:58.977633920Z"
    }
   },
   "id": "f33340deaf7665f7"
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "model1 = ModelV1(in_features=37)\n",
    "\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(params=model1.parameters(), lr=0.0001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T16:23:49.219812742Z",
     "start_time": "2024-07-29T16:23:49.211783634Z"
    }
   },
   "id": "cfa7ae65ba16ce"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 843.7307,  915.1597, 1170.9192,  ..., 1338.3646, 1107.5521,\n        1050.4900], grad_fn=<SqueezeBackward0>)"
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.eval()\n",
    "y_pred = model1(X_train).squeeze()\n",
    "y_pred"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T16:23:49.572265994Z",
     "start_time": "2024-07-29T16:23:49.559590004Z"
    }
   },
   "id": "1534193b0b21b3c5"
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "def train_test(model:torch.nn.Module, loss_fn:torch.nn.Module, optimizer:torch.optim.Optimizer, epochs:int, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        y_preds = model(X_train).squeeze()\n",
    "        loss = loss_fn(y_preds, y_train)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with (torch.inference_mode()):\n",
    "            test_pred = model(X_test).squeeze()\n",
    "            test_loss = loss_fn(test_pred, y_test)\n",
    "        if epoch % 50 == 0:\n",
    "            print(\"----------------------------\")\n",
    "            print(f\"Epoch {epoch}:\")\n",
    "            print(f\"Train loss: {loss} | Test loss: {test_loss}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T16:23:49.963126177Z",
     "start_time": "2024-07-29T16:23:49.960973852Z"
    }
   },
   "id": "99076809838fcd50"
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "Epoch 0:\n",
      "Train loss: 178740.296875 | Test loss: 171537.421875\n",
      "----------------------------\n",
      "Epoch 50:\n",
      "Train loss: 59481.9921875 | Test loss: 57938.1640625\n",
      "----------------------------\n",
      "Epoch 100:\n",
      "Train loss: 54991.64453125 | Test loss: 54821.1171875\n",
      "----------------------------\n",
      "Epoch 150:\n",
      "Train loss: 51404.84765625 | Test loss: 52224.13671875\n",
      "----------------------------\n",
      "Epoch 200:\n",
      "Train loss: 48755.9296875 | Test loss: 50324.21484375\n",
      "----------------------------\n",
      "Epoch 250:\n",
      "Train loss: 46787.62890625 | Test loss: 48741.765625\n",
      "----------------------------\n",
      "Epoch 300:\n",
      "Train loss: 45368.17578125 | Test loss: 47503.1328125\n",
      "----------------------------\n",
      "Epoch 350:\n",
      "Train loss: 44312.11328125 | Test loss: 46636.7578125\n",
      "----------------------------\n",
      "Epoch 400:\n",
      "Train loss: 43373.04296875 | Test loss: 45797.54296875\n",
      "----------------------------\n",
      "Epoch 450:\n",
      "Train loss: 42592.984375 | Test loss: 45001.95703125\n",
      "----------------------------\n",
      "Epoch 500:\n",
      "Train loss: 41882.8828125 | Test loss: 44292.9375\n",
      "----------------------------\n",
      "Epoch 550:\n",
      "Train loss: 41253.66796875 | Test loss: 43559.08203125\n",
      "----------------------------\n",
      "Epoch 600:\n",
      "Train loss: 40745.05078125 | Test loss: 42974.546875\n",
      "----------------------------\n",
      "Epoch 650:\n",
      "Train loss: 40255.74609375 | Test loss: 42415.59765625\n",
      "----------------------------\n",
      "Epoch 700:\n",
      "Train loss: 39783.890625 | Test loss: 41940.50390625\n",
      "----------------------------\n",
      "Epoch 750:\n",
      "Train loss: 39326.3203125 | Test loss: 41488.8359375\n",
      "----------------------------\n",
      "Epoch 800:\n",
      "Train loss: 38885.27734375 | Test loss: 40949.40234375\n",
      "----------------------------\n",
      "Epoch 850:\n",
      "Train loss: 38458.40234375 | Test loss: 40451.7734375\n",
      "----------------------------\n",
      "Epoch 900:\n",
      "Train loss: 38050.5 | Test loss: 39951.61328125\n",
      "----------------------------\n",
      "Epoch 950:\n",
      "Train loss: 37652.07421875 | Test loss: 39473.953125\n",
      "----------------------------\n",
      "Epoch 1000:\n",
      "Train loss: 37266.98046875 | Test loss: 39011.22265625\n",
      "----------------------------\n",
      "Epoch 1050:\n",
      "Train loss: 36888.14453125 | Test loss: 38550.6171875\n",
      "----------------------------\n",
      "Epoch 1100:\n",
      "Train loss: 36522.1640625 | Test loss: 38124.34765625\n",
      "----------------------------\n",
      "Epoch 1150:\n",
      "Train loss: 36174.6875 | Test loss: 37697.5703125\n",
      "----------------------------\n",
      "Epoch 1200:\n",
      "Train loss: 35850.3046875 | Test loss: 37306.20703125\n",
      "----------------------------\n",
      "Epoch 1250:\n",
      "Train loss: 35538.3046875 | Test loss: 36902.84765625\n",
      "----------------------------\n",
      "Epoch 1300:\n",
      "Train loss: 35235.29296875 | Test loss: 36527.74609375\n",
      "----------------------------\n",
      "Epoch 1350:\n",
      "Train loss: 34941.70703125 | Test loss: 36161.7734375\n",
      "----------------------------\n",
      "Epoch 1400:\n",
      "Train loss: 34665.40234375 | Test loss: 35809.40234375\n",
      "----------------------------\n",
      "Epoch 1450:\n",
      "Train loss: 34395.78515625 | Test loss: 35476.0\n",
      "----------------------------\n",
      "Epoch 1500:\n",
      "Train loss: 34136.25 | Test loss: 35146.70703125\n",
      "----------------------------\n",
      "Epoch 1550:\n",
      "Train loss: 33887.95703125 | Test loss: 34827.2109375\n",
      "----------------------------\n",
      "Epoch 1600:\n",
      "Train loss: 33652.6328125 | Test loss: 34513.19140625\n",
      "----------------------------\n",
      "Epoch 1650:\n",
      "Train loss: 33427.578125 | Test loss: 34247.41015625\n",
      "----------------------------\n",
      "Epoch 1700:\n",
      "Train loss: 33216.15234375 | Test loss: 33959.12109375\n",
      "----------------------------\n",
      "Epoch 1750:\n",
      "Train loss: 33012.34765625 | Test loss: 33672.32421875\n",
      "----------------------------\n",
      "Epoch 1800:\n",
      "Train loss: 32812.43359375 | Test loss: 33424.74609375\n",
      "----------------------------\n",
      "Epoch 1850:\n",
      "Train loss: 32618.982421875 | Test loss: 33165.79296875\n",
      "----------------------------\n",
      "Epoch 1900:\n",
      "Train loss: 32428.58984375 | Test loss: 32924.62890625\n",
      "----------------------------\n",
      "Epoch 1950:\n",
      "Train loss: 32247.24609375 | Test loss: 32689.869140625\n"
     ]
    }
   ],
   "source": [
    "train_test(model=model1, loss_fn=loss_fn, optimizer=optimizer, epochs=2000, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T16:23:51.878033587Z",
     "start_time": "2024-07-29T16:23:50.778297295Z"
    }
   },
   "id": "cf68177afdf2c915"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## This results are not great, let's figure out which 5 data columns have the most corelation with SalePrice and use only them"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86fb12508d61e8e5"
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T16:23:58.194851707Z",
     "start_time": "2024-07-29T16:23:58.192740390Z"
    }
   },
   "id": "e8e7e783399b8c90"
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "data": {
      "text/plain": "4     0.559843\n16    0.482892\n12    0.366936\n26    0.366470\n27    0.364180\n6     0.363803\n13    0.310731\nName: MI Scores, dtype: float64"
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discrete_features = X_num.dtypes == int\n",
    "\n",
    "mi_scores = make_mi_scores(X_num, y, discrete_features=discrete_features)\n",
    "mi_scores[:7]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T16:25:42.134939910Z",
     "start_time": "2024-07-29T16:25:41.953113603Z"
    }
   },
   "id": "2e7b7d286e2e9220"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's now use only this 7 features in a Model and see if there is any difference in performance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d35d8f0f804fcc5"
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "model2 = ModelV1(in_features=7)\n",
    "X_num_2 = X_num[list(mi_scores[:7].index)]\n",
    "X_train, X_test, y_train, y_test = get_train_test_data(X_num_2, y, test_size=0.2)\n",
    "optimizer = torch.optim.SGD(params=model2.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T16:25:47.164426743Z",
     "start_time": "2024-07-29T16:25:47.156704302Z"
    }
   },
   "id": "669c09d2f00993d0"
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "Epoch 0:\n",
      "Train loss: 180676.6875 | Test loss: 175690.125\n",
      "----------------------------\n",
      "Epoch 50:\n",
      "Train loss: 37957.44140625 | Test loss: 40463.37109375\n",
      "----------------------------\n",
      "Epoch 100:\n",
      "Train loss: 36233.81640625 | Test loss: 38626.58203125\n",
      "----------------------------\n",
      "Epoch 150:\n",
      "Train loss: 34877.3515625 | Test loss: 37109.23828125\n",
      "----------------------------\n",
      "Epoch 200:\n",
      "Train loss: 33828.75390625 | Test loss: 35856.296875\n",
      "----------------------------\n",
      "Epoch 250:\n",
      "Train loss: 33010.765625 | Test loss: 34834.5625\n",
      "----------------------------\n",
      "Epoch 300:\n",
      "Train loss: 32319.75 | Test loss: 33934.6328125\n",
      "----------------------------\n",
      "Epoch 350:\n",
      "Train loss: 31775.119140625 | Test loss: 33301.6875\n",
      "----------------------------\n",
      "Epoch 400:\n",
      "Train loss: 31368.0234375 | Test loss: 32761.423828125\n",
      "----------------------------\n",
      "Epoch 450:\n",
      "Train loss: 31063.201171875 | Test loss: 32358.5078125\n",
      "----------------------------\n",
      "Epoch 500:\n",
      "Train loss: 30808.787109375 | Test loss: 32044.349609375\n",
      "----------------------------\n",
      "Epoch 550:\n",
      "Train loss: 30583.74609375 | Test loss: 31778.572265625\n",
      "----------------------------\n",
      "Epoch 600:\n",
      "Train loss: 30400.595703125 | Test loss: 31547.640625\n",
      "----------------------------\n",
      "Epoch 650:\n",
      "Train loss: 30241.951171875 | Test loss: 31343.58984375\n",
      "----------------------------\n",
      "Epoch 700:\n",
      "Train loss: 30106.650390625 | Test loss: 31175.544921875\n",
      "----------------------------\n",
      "Epoch 750:\n",
      "Train loss: 29990.19921875 | Test loss: 31031.08203125\n",
      "----------------------------\n",
      "Epoch 800:\n",
      "Train loss: 29889.951171875 | Test loss: 30928.259765625\n",
      "----------------------------\n",
      "Epoch 850:\n",
      "Train loss: 29800.0234375 | Test loss: 30866.900390625\n",
      "----------------------------\n",
      "Epoch 900:\n",
      "Train loss: 29719.609375 | Test loss: 30756.28515625\n",
      "----------------------------\n",
      "Epoch 950:\n",
      "Train loss: 29644.373046875 | Test loss: 30678.626953125\n",
      "----------------------------\n",
      "Epoch 1000:\n",
      "Train loss: 29576.986328125 | Test loss: 30589.64453125\n",
      "----------------------------\n",
      "Epoch 1050:\n",
      "Train loss: 29514.369140625 | Test loss: 30505.5625\n",
      "----------------------------\n",
      "Epoch 1100:\n",
      "Train loss: 29454.900390625 | Test loss: 30441.6640625\n",
      "----------------------------\n",
      "Epoch 1150:\n",
      "Train loss: 29401.1171875 | Test loss: 30389.146484375\n",
      "----------------------------\n",
      "Epoch 1200:\n",
      "Train loss: 29353.28515625 | Test loss: 30340.6171875\n",
      "----------------------------\n",
      "Epoch 1250:\n",
      "Train loss: 29308.822265625 | Test loss: 30267.14453125\n",
      "----------------------------\n",
      "Epoch 1300:\n",
      "Train loss: 29265.7578125 | Test loss: 30217.705078125\n",
      "----------------------------\n",
      "Epoch 1350:\n",
      "Train loss: 29226.109375 | Test loss: 30164.9140625\n",
      "----------------------------\n",
      "Epoch 1400:\n",
      "Train loss: 29192.35546875 | Test loss: 30115.671875\n",
      "----------------------------\n",
      "Epoch 1450:\n",
      "Train loss: 29159.8828125 | Test loss: 30064.740234375\n",
      "----------------------------\n",
      "Epoch 1500:\n",
      "Train loss: 29128.732421875 | Test loss: 30011.681640625\n",
      "----------------------------\n",
      "Epoch 1550:\n",
      "Train loss: 29100.13671875 | Test loss: 29977.14453125\n",
      "----------------------------\n",
      "Epoch 1600:\n",
      "Train loss: 29073.6328125 | Test loss: 29940.576171875\n",
      "----------------------------\n",
      "Epoch 1650:\n",
      "Train loss: 29049.841796875 | Test loss: 29913.271484375\n",
      "----------------------------\n",
      "Epoch 1700:\n",
      "Train loss: 29028.849609375 | Test loss: 29880.595703125\n",
      "----------------------------\n",
      "Epoch 1750:\n",
      "Train loss: 29010.00390625 | Test loss: 29868.03515625\n",
      "----------------------------\n",
      "Epoch 1800:\n",
      "Train loss: 28992.185546875 | Test loss: 29846.5\n",
      "----------------------------\n",
      "Epoch 1850:\n",
      "Train loss: 28975.36328125 | Test loss: 29825.328125\n",
      "----------------------------\n",
      "Epoch 1900:\n",
      "Train loss: 28958.94921875 | Test loss: 29809.291015625\n",
      "----------------------------\n",
      "Epoch 1950:\n",
      "Train loss: 28943.7265625 | Test loss: 29805.076171875\n"
     ]
    }
   ],
   "source": [
    "train_test(model=model2, loss_fn=loss_fn, optimizer=optimizer, epochs=2000, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T16:25:48.764398989Z",
     "start_time": "2024-07-29T16:25:48.334572910Z"
    }
   },
   "id": "84eeee795b86b4c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-29T16:10:17.817604602Z"
    }
   },
   "id": "9e1de00f8245b7b4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
